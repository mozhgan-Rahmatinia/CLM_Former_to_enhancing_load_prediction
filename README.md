# CLM-Former: Enhancing Multi-Horizon Load Forecasting in Smart Microgrids

This repository provides the official implementation of the paper:

**CLM-Former for Enhancing Multi-Horizon Time Series Forecasting and Load Prediction in Smart Microgrids Using a Robust Transformer-Based Model**

ğŸ“Œ *Scientific Reports (Nature Portfolio) â€“ Q1 Journal*  
ğŸ“Œ *Open Access*

---

## ğŸ“„ Paper Information

- **Journal:** Scientific Reports (Nature)
- **DOI:** https://doi.org/10.1038/s41598-025-34870-y
- **Full Text (Open Access):**  
  https://www.nature.com/articles/s41598-025-34870-y
- **Authors:**  
  - S. Mozhgan Rahmatinia  
  - Seyed-Majid Hosseini  
  - Seyed-Amin Hosseini-Seno  

---

## ğŸ§  Abstract

Accurate multi-horizon load forecasting is a cornerstone of efficient and reliable smart microgrid operation, particularly in residential environments characterized by strong seasonality and abrupt short-term fluctuations.

To address the limitations of existing Transformer-based forecasting models, this work proposes **CLM-Former**, a novel hybrid deep learning architecture that enhances the Autoformer framework by integrating a **CNNâ€“LSTM subnetwork (CLM-subNet)** directly into the decomposition-based Transformer blocks.

CLM-Former effectively captures:
- Long-term periodic patterns via frequency-domain autocorrelation
- Short-term and localized fluctuations via hierarchical time-domain modeling

Extensive experiments on real-world residential electricity consumption datasets demonstrate that CLM-Former consistently outperforms state-of-the-art Transformer-based and deep learning models across multiple forecasting horizons, while preserving computational efficiency.

---

## âœ¨ Key Contributions

- ğŸ”¹ **Architectural Innovation:**  
  Replaces the point-wise Feed-Forward Network in Autoformer with a learnable **CNNâ€“LSTM subnetwork** operating on the seasonal component.

- ğŸ”¹ **Hybrid Temporal Modeling:**  
  Combines frequency-domain autocorrelation (long-term periodicity) with time-domain convolutional and recurrent modeling (short-term dynamics).

- ğŸ”¹ **Multi-Horizon Forecasting:**  
  Accurate predictions for short-, medium-, and long-term horizons (96, 192, 336 steps).

- ğŸ”¹ **Global Multivariate Forecasting:**  
  A single model jointly forecasts electricity consumption for **321 residential households**.

- ğŸ”¹ **Generalizability:**  
  Validated on Electricity, Traffic, ETTm2, and Weather benchmark datasets.

- ğŸ”¹ **Efficiency-Preserving Design:**  
  Maintains Autoformerâ€™s \(O(L \log L)\) complexity with minimal inference overhead.

---

## ğŸ—ï¸ CLM-Former Architecture

CLM-Former follows an **Encoderâ€“Decoder Transformer architecture** with progressive time series decomposition.

### ğŸ”¹ Core Components

1. **Series Decomposition (Layer-wise)**  
   Splits the input into:
   - Seasonal component
   - Cyclicalâ€“trend component

2. **Autocorrelation-Based Attention**  
   Captures long-range dependencies and dominant periodic patterns in the frequency domain.

3. **CLM-subNet (CNN + LSTM)**  
   Applied exclusively to the seasonal component to:
   - Extract local high-frequency patterns (CNN)
   - Model sequential temporal dependencies (LSTM)

4. **Progressive Aggregation**  
   Trend components are accumulated while refined seasonal signals generate multi-horizon forecasts.

---

## ğŸ–¼ï¸ Model Overview


![CLM-Former Overview](figures/fig1_clmformer_overview.png)


Conceptual overview of CLM-Former illustrating decomposition, autocorrelation, and CLM-subNet integration.

![CLM-Former Architecture](figures/fig3_architecture.png)


Encoderâ€“Decoder architecture with CLM-subNet replacing the Feed-Forward Network.

![CLM-subNet](figures/fig4_clm_subnet.png)

Internal structure of the proposed CNNâ€“LSTM CLM-subNet.

ğŸ“Š Datasets
Primary Dataset

Electricity Dataset

321 residential households

Hourly resolution

3 years (2016â€“2019)

Additional Benchmarks

Traffic

ETTm2

Weather

Data Split:

70% Training

10% Validation

20% Testing

ğŸ“ˆ Evaluation Metrics

Mean Squared Error (MSE)

Mean Absolute Error (MAE)

Statistical significance validated via:

Paired t-test

Wilcoxon signed-rank test

ğŸ§ª Experimental Results

Consistent superiority over:

Autoformer

Crossformer

Informer

Reformer

Transformer

CNN-LSTM, LSTM, TCN, SCINet, TiDE

Strong gains at longer horizons (336 steps)

Improved accuracy without sacrificing inference speed



Qualitative comparison of CLM-Former against Transformer-based baselines.

âš™ï¸ Computational Efficiency

Retains 
ğ‘‚
(
ğ¿
log
â¡
ğ¿
)
complexity

Comparable inference time to Autoformer

Fewer parameters than convolution-only Autoformer variants due to LSTM bottleneck design

ğŸ–¥ï¸ Environment

Python 3.11

PyTorch â‰¥ 2.2

NumPy, Pandas

GPU: NVIDIA RTX / T4 (tested)

